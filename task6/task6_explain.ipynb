{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 - https://excelsior-cjh.tistory.com/93 (간단한 TextRank 실제 구현해보기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 문서 타입에 따른 문장 단위로 분리\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.twitter = Twitter()\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "             ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]\n",
    "    \n",
    "    # newspaper 패키지를 이용하여 기사 텍스트 크롤링\n",
    "    def url2sentences(self, url):\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        sentences = self.kkma.sentences(article.text)\n",
    "        \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        \n",
    "        return sentences\n",
    "  \n",
    "    # 문장 분리\n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)      \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        \n",
    "        return sentences\n",
    "\n",
    "    # SentenceTokenizer 클래스로 명사 추출 ('__init__(self)'부분)\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence != '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence)) \n",
    "                                       if noun not in self.stopwords and len(noun) > 1]))\n",
    "        \n",
    "        return nouns\n",
    "    \n",
    "    \n",
    "# TF-IDF 모델 생성 및 그래프 생성 과정\n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()   # from sklearn.feature_extraction.text\n",
    "        self.cnt_vec = CountVectorizer()   # from sklearn.feature_extraction.text\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    # 명사로 이루어진 문장을 입력받아 tfidf matrix를 만든 후 Sentence graph를 반환\n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return  self.graph_sentence\n",
    "        \n",
    "    # 명사로 이루어진 문장을 입력받아 matrix를 만든 후 word graph와 {idx: word}형태의 dictionary를 반환\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    \n",
    "    \n",
    "# TextRank 알고리즘 구현\n",
    "# 앞에서 생성된 문장(or 단어)의 가중치 그래프를 이용하여 TextRank 알고리즘 적용\n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로 \n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}   # {idx : rank 값} 형태의 dictionary를 반환\n",
    "    \n",
    "    \n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "                    \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx =  self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "    \n",
    "    # 요약\n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    # 키워드 추출\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        \n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 다음 뉴스 기사 1개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://v.daum.net/v/20231103215437812'    # 기사 링크\n",
    "textrank = TextRank(url)    # TextRank class 실행\n",
    "for row in textrank.summarize(4):   # 4줄로 요악, 1줄씩 출력\n",
    "    print(row)\n",
    "    print()\n",
    "print('keywords :',textrank.keywords())    # textrank의 키워드 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행 결과: \n",
    "\n",
    "미국의 지난달 비농업 부문 신규 고용이 15만 건을 기록 해 시장 예상치를 크게 하회했다.\n",
    "\n",
    "<w/>\n",
    "\n",
    "동시에 실업률은 9월보다 0.1% 포인트 올라 지난해 1월 이후 최고치를 찍었다.\n",
    "\n",
    "<w/>\n",
    "\n",
    "미국 노동부는 10월 비농업 부문 고용 건수가 15만 건을 기록했다고\n",
    "\n",
    "<w/>\n",
    "\n",
    "함께 발표된 지난달 실업률은 3.9% 로 예상치 (3.8% )를 상회했고 지난해 1월 (4%) 이후 가장 높았다\n",
    ".\n",
    "\n",
    "<w/>\n",
    "\n",
    "keywords : ['상치', '지난달', '실업률', '지난해', '이후', '고용', '미국', '기록', '농업', '부 \n",
    "문']\n",
    "\n",
    "<w/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 이미 모은 텍스트 데이터 1개 요약\n",
    "SentenceTokenizer Class의 url2sentences 함수 부분 생략하고 실행해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 \n",
    "1. 웹크롤링 부분을 제외하고 \n",
    "2. 미리 수집한 pdf에서 텍스트를 추출하여 (https://rfriend.tistory.com/790)\n",
    "요약을 시행하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "class SentenceTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "        self.twitter = Twitter()\n",
    "        self.stopwords = ['중인' ,'만큼', '마찬가지', '꼬집었', \"연합뉴스\", \"데일리\", \"동아일보\", \"중앙일보\", \"조선일보\", \"기자\"\n",
    "             ,\"아\", \"휴\", \"아이구\", \"아이쿠\", \"아이고\", \"어\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\",]\n",
    "    \n",
    "  \n",
    "    def text2sentences(self, text):\n",
    "        sentences = self.kkma.sentences(text)      \n",
    "        for idx in range(0, len(sentences)):\n",
    "            if len(sentences[idx]) <= 10:\n",
    "                sentences[idx-1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = ''\n",
    "        \n",
    "        return sentences\n",
    "\n",
    "    def get_nouns(self, sentences):\n",
    "        nouns = []\n",
    "        for sentence in sentences:\n",
    "            if sentence != '':\n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence)) \n",
    "                                       if noun not in self.stopwords and len(noun) > 1]))\n",
    "        \n",
    "        return nouns\n",
    "    \n",
    "    \n",
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return  self.graph_sentence\n",
    "        \n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "    \n",
    "    \n",
    "    \n",
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로 \n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "            \n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "    \n",
    "    \n",
    "\n",
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        \n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        \n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "                    \n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        \n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        \n",
    "        self.word_rank_idx =  self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        \n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        \n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "            \n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "\n",
    "\n",
    "pdf_path = \"test_p.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "text_all = []\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    text_all.append(text)\n",
    "    \n",
    "for idx in range(0, len(text_all)):\n",
    "    if len(text_all[idx]) <= 100:\n",
    "        text_all[idx-1] += (' ' + text_all[idx])\n",
    "        text_all[idx] = ''\n",
    "    \n",
    "\n",
    "# filename = \"4차 산업 혁명 시대, 대학 교육과 콘텐츠.pdf\";\n",
    "# str = extractFileText(filename)\n",
    "textrank = TextRank(text_all)\n",
    "for row in textrank.summarize(4):\n",
    "    print(row)\n",
    "    print()\n",
    "print('keywords :',textrank.keywords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FileNotFoundError: [Errno 2] No such file or directory: 'test_p.pdf'\n",
    "\n",
    "<w/>\n",
    "\n",
    "파일을 찾지 못하는 에러가 나서 코드 수정이 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 요약 품질 소감\n",
    "\n",
    "짧은 텍스트 (신문 기사, 1)의 경우 중요한 내용만 잘 요약하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 해볼 것\n",
    "https://lovit.github.io/nlp/2019/04/30/textrank/\n",
    "참고하여 textRank 구현 부분 수정"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
