## 1. 목표 사이트
링커리어(https://linkareer.com/list/intern?gclid=Cj0KCQjwmICoBhDxARIsABXkXlJlZAGt_Dy-1h6-DfBfDp-92-wbiK-3c--1z5vTkquOLp-Ru0IMJhUaAnVqEALw_wcB)
-> 대외활동 사이트 올콘 (https://www.all-con.co.kr/)
변경

## 2. 내가 공부한 crawling 기법들(특정 블로그에 적힌 여러가지 기법)
### BeautifulSoup
아래와 강의자료실에 있는 3개의 블로그에 나와 있는 코드를 참고하여 1차 코드를 작성, 후에 table 구조를 확인하며 전부 엎었다.
- ().select(“ “)
- ().select_one(“ “)
- ().text
- ().get(“ “)
- .find(“ “)
- .find_all(“ “)
- [BeautifulSoup SELECT 정리 및 사용법](https://pythonblog.co.kr/coding/11/)
- [[Python 크롤링] 2. Beautiful Soup, bs4 사용법, find(), find_all(), select()](https://parkjh7764.tistory.com/139)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

아래 두 개의 코드를 참고하여 2차 코드 작성, 후에 Selenium을 사용하며 전부 엎었다.
- [HTML tr tag](https://www.w3schools.com/tags/tag_tr.asp)
- [HTML 테이블 구조 (Python 웹크롤링)](https://greendreamtrre.tistory.com/194)

- lxml과 html.parser의 차이점
  - lxml이 html.parser보다 빠르게 동작한다. 이외에도 단일 구문 처리라던가, html로 마크업 되어 있지 않은 경우라던가... 많은 부분에서 유용해서 따로 lxml을 설치하여 사용한다.

### Selenium

## 3. 목표 사이트를 접근하기 위해서 수정한 코드 설명

## 4. 최종 DataFrame에 넣고, 저장

## 5. 데이터 크롤링 구상
